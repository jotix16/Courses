{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from RNN import *\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Checking the mapper and predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()\n",
    "print(rnn.mapper[rnn.mapper[\"Passion\"]]) # from string to array of one hot-encoded columns and back\n",
    "print(rnn.mapper[1]) # from index to char\n",
    "one_hot = rnn.mapper[np.array([1,2,5,77,3,1,2,0])]# from list of indexes to array of one hot-encoded vectors\n",
    "print(rnn.word(one_hot))\n",
    "\n",
    "a = rnn.predict2(x=rnn.mapper[\"Harry\"], h=np.ones(100), n=4)\n",
    "print(a.shape) # array with hot encoded columns\n",
    "print(rnn.word(a)) #word and mapper are same\n",
    "print(len(rnn.mapper[a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Check gradients numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALCULATING NUMERICAL GRADIENTS\n",
      "Sum of relative weights error for the b : 2.0101982766669755e-08\n",
      "Sum of relative weights error for the c : 1.1380771223064707e-07\n",
      "Sum of relative weights error for the W : 4.1351267983285206e-07\n",
      "Sum of relative weights error for the V : 1.1774052302721346e-06\n",
      "Sum of relative weights error for the U : 2.799743233438285e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxdox/.local/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "rnn_check = RNN(m=5)\n",
    "h0 = np.random.rand(5,1)\n",
    "seq_length = 25\n",
    "# words                              # one hot\n",
    "X_chars= book_data[:seq_length];     X = rnn_check.mapper[X_chars]\n",
    "Y_chars = book_data[1:seq_length+1]; Y = rnn_check.mapper[Y_chars]\n",
    "\n",
    "grad_b_, grad_c_, grad_U_, grad_W_, grad_V_ = comp_gradients(rnn_check, X, Y, h0)\n",
    "grad_b, grad_c, grad_U, grad_W, grad_V = ComputeGradsNumSlow(rnn_check,X, Y, h0,1e-4)\n",
    "\n",
    "## grad_b\n",
    "maks_bias = np.max( np.abs(np.concatenate((grad_b[np.newaxis,:],grad_b_[np.newaxis,:]), \n",
    "                  axis = 0)),axis = 0)\n",
    "err_bias = np.nansum(np.abs(grad_b - grad_b_)/maks_bias)\n",
    "print(\"Sum of relative weights error for the b :\",err_bias)\n",
    "\n",
    "## grad_c\n",
    "maks_bias = np.max( np.abs(np.concatenate((grad_c[np.newaxis,:],grad_c_[np.newaxis,:]), \n",
    "                  axis = 0)),axis = 0)\n",
    "err_bias = np.nansum(np.abs(grad_c - grad_c_)/maks_bias)\n",
    "print(\"Sum of relative weights error for the c :\",err_bias)\n",
    "\n",
    "#grad W\n",
    "maks_weight = np.max(np.abs(np.concatenate((grad_W[np.newaxis,:,:], grad_W_[np.newaxis,:,:]),\n",
    "                                           axis = 0)),axis = 0)\n",
    "err_weights = np.nansum(np.abs(grad_W - grad_W_)/maks_weight)\n",
    "print(\"Sum of relative weights error for the W :\",err_weights)\n",
    "\n",
    "#grad V\n",
    "maks_weight = np.max(np.abs(np.concatenate((grad_V[np.newaxis,:,:], grad_V_[np.newaxis,:,:]),\n",
    "                                           axis = 0)),axis = 0)\n",
    "err_weights = np.nansum(np.abs(grad_V - grad_V_)/maks_weight)\n",
    "print(\"Sum of relative weights error for the V :\",err_weights)\n",
    "\n",
    "#grad U\n",
    "maks_weight = np.max(np.abs(np.concatenate((grad_U[np.newaxis,:,:], grad_U_[np.newaxis,:,:]),\n",
    "                                           axis = 0)),axis = 0)\n",
    "err_weights = np.nansum(np.abs(grad_U - grad_U_)/maks_weight)\n",
    "print(\"Sum of relative weights error for the U :\",err_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)a Train RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 44159/44301 [08:46<00:01, 97.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.344818199542225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44301/44301 [08:47<00:00, 83.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "rnn = RNN()\n",
    "seq_length = 25\n",
    "# words                              # one hot\n",
    "X_chars= book_data[:seq_length];     X = rnn.mapper[X_chars]\n",
    "Y_chars = book_data[1:seq_length+1]; Y = rnn.mapper[Y_chars]\n",
    "# init hidden state\n",
    "h0 = np.zeros((100,1))\n",
    "loss = rnn.backward(X, Y, h0)\n",
    "print(loss)\n",
    "ll = 0\n",
    "\n",
    "# training loop\n",
    "for k in range(8):\n",
    "    for i in tqdm(range(0,len(book_data)-25,seq_length)):\n",
    "            \n",
    "        # words                                  # one hot\n",
    "        X_chars= book_data[i:i+seq_length];      X = rnn.mapper[X_chars]\n",
    "        Y_chars = book_data[i+1:i+seq_length+1]; Y = rnn.mapper[Y_chars]\n",
    "        \n",
    "        # display loss every 250th update\n",
    "        if np.mod(ll,250)==0:\n",
    "            clear_output(wait=True)\n",
    "            print(loss) \n",
    "        \n",
    "        # synthesize a text of length 250 letters every 500th update\n",
    "        if np.mod(ll,500)==0:\n",
    "            txt = rnn.predict(x=X[:,0], h=rnn.h, n=250)\n",
    "            if np.mod(ll,1000)==0: \n",
    "                with open('out.txt', 'a') as f:\n",
    "                    print(\"\\n*iter =*\" +str(ll)+\"*, smooth_loos=*\"+str(loss)+\"\\n\", file=f)\n",
    "                    print(\"\".join(rnn.word(txt)), file=f)\n",
    "            printmd(\"**iter =**\" +str(ll)+\"**, smooth_loos=**\"+str(loss)+\"\\n\" )\n",
    "            print(\"\".join(rnn.word(txt)))\n",
    "            \n",
    "        # Reset init hidden state every new epoche\n",
    "        if i == 0:\n",
    "            rnn.h=None\n",
    "        loss = 0.999*loss + 0.001*rnn.backward(X, Y, h0); ll +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)bSynthesize a text of length 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = rnn.predict(x=rnn.mapper[\" \"], h=np.zeros((100,1)), n=2000)\n",
    "text = \"\".join(rnn.word(tmp))\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
