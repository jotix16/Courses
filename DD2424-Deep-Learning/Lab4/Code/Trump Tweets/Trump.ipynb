{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from RNN import *\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "from trump_tweets import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_zip_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master_2015.json\n",
      "master_2017.json\n",
      "master_2012.json\n",
      "master_2014.json\n",
      "master_2010.json\n",
      "master_2018.json\n",
      "master_2011.json\n",
      "master_2009.json\n",
      "master_2016.json\n",
      "master_2013.json\n"
     ]
    }
   ],
   "source": [
    "data = read_json_text_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = token_segmentation(data) # remove speicial charackters from the tweets and add <at the end of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9', 'q', '?', 'V', 't', 'Y', '!', 'W', 'h', '6', 'o', 'f', '2', '4', '5', '1', 'i', 'y', 'T', 'R', 'M', 'C', 'm', 'Q', 'p', 's', 'x', 'X', 'v', 'B', '#', 'N', 'l', 'u', '.', 'J', 'F', 'O', '@', 'a', ',', 'P', '3', 'E', 'j', 'g', 'I', 'S', 'e', 'd', 'H', 'D', 'r', 'k', 'Z', 'n', ' ', '8', 'L', '7', 'A', 'b', 'w', '0', 'G', 'z', 'K', 'c', \"'\", '<', 'U'}\n"
     ]
    }
   ],
   "source": [
    "all_data = \"\"\n",
    "for tweet in ret:\n",
    "    all_data+=tweet\n",
    "\n",
    "book_chars = {uniq for uniq in all_data} # create letter dictionary\n",
    "print(book_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 32134/32294 [11:08<00:02, 79.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.19470162268038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32294/32294 [11:10<00:00, 48.17it/s]\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(book_chars=book_chars)\n",
    "seq_length=25\n",
    "X_chars= all_data[0:seq_length]\n",
    "Y_chars = all_data[1:seq_length+1]\n",
    "# one hot\n",
    "X = rnn.mapper[X_chars];\n",
    "Y = rnn.mapper[Y_chars];\n",
    "h0 = np.zeros((100,1))\n",
    "\n",
    "\n",
    "loss = rnn.backward(X, Y, h0)\n",
    "print(loss)\n",
    "ll = 0\n",
    "for k in range(18):\n",
    "    for tweet in tqdm(ret):\n",
    "\n",
    "        seq_length = random.randint(1, len(tweet)-1)\n",
    "        if seq_length < 5: continue\n",
    "        \n",
    "        X_chars= tweet[0:seq_length]; X = rnn.mapper[X_chars]\n",
    "        Y_chars = tweet[1:seq_length+1]; Y = rnn.mapper[Y_chars]       \n",
    "        \n",
    "        if np.mod(ll,250)==0:\n",
    "            clear_output(wait=True)\n",
    "            print(loss) \n",
    "        \n",
    "        if np.mod(ll,500)==0:\n",
    "            txt = rnn.predict(x=X[:,0], h=rnn.h, n=250)\n",
    "            if np.mod(ll,1000)==0: \n",
    "                with open('trump_synthezed/out_trum3.txt', 'a') as f:\n",
    "                    print(\"\\n*iter =*\" +str(ll)+\"*, smooth_loos=*\"+str(loss)+\"\\n\", file=f)\n",
    "                    print(\"\".join(rnn.word(txt)), file=f)\n",
    "            printmd(\"**iter =**\" +str(ll)+\"**, smooth_loos=**\"+str(loss)+\"\\n\" )\n",
    "            print(\"\".join(rnn.word(txt)))\n",
    "            \n",
    "        ##\n",
    "        rnn.h=None\n",
    "        loss = 0.999*loss + 0.001*rnn.backward(X, Y, h0)\n",
    "            \n",
    "\n",
    "        ll +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 32276/32294 [32:36<00:00, 18.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.10089705479548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32294/32294 [32:37<00:00, 16.50it/s]\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(book_chars=book_chars)\n",
    "seq_length=10\n",
    "X_chars= all_data[0:seq_length]\n",
    "Y_chars = all_data[1:seq_length+1]\n",
    "# one hot\n",
    "X = rnn.mapper[X_chars];\n",
    "Y = rnn.mapper[Y_chars];\n",
    "h0 = np.zeros((100,1))\n",
    "\n",
    "\n",
    "loss = rnn.backward(X, Y, h0)\n",
    "print(loss)\n",
    "ll = 0\n",
    "for k in range(3):\n",
    "    for tweet in tqdm(ret):\n",
    "\n",
    "        for iter in range(0,len(tweet),seq_length):\n",
    "            if iter+seq_length+1>= len(tweet): continue\n",
    "                \n",
    "            X_chars= tweet[iter:iter+seq_length]; X = rnn.mapper[X_chars]\n",
    "            Y_chars = tweet[iter+1:iter+seq_length+1]; Y = rnn.mapper[Y_chars]       \n",
    "\n",
    "            if np.mod(ll,250)==0:\n",
    "                clear_output(wait=True)\n",
    "                print(loss) \n",
    "\n",
    "            if np.mod(ll,500)==0:\n",
    "                txt = rnn.predict(x=X[:,0], h=rnn.h, n=250)\n",
    "                if np.mod(ll,1000)==0: \n",
    "                    with open('trump_synthezed/short10.txt', 'a') as f:\n",
    "                        print(\"\\n*iter =*\" +str(ll)+\"*, smooth_loos=*\"+str(loss)+\"\\n\", file=f)\n",
    "                        print(\"\".join(rnn.word(txt)), file=f)\n",
    "                printmd(\"**iter =**\" +str(ll)+\"**, smooth_loos=**\"+str(loss)+\"\\n\" )\n",
    "                print(\"\".join(rnn.word(txt)))\n",
    "\n",
    "            if iter==0: rnn.h=None\n",
    "            loss = 0.999*loss + 0.001*rnn.backward(X, Y, h0)\n",
    "            ll +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize 10 tweets starting from empty hprev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@reWeniasing watt The yol... I Hewist have @realDonaldTrump tax Myo. @FoxidJPapluycol the. ExThes is vich erld whos!<am. C oute, you 't  to \n",
      "\n",
      "\n",
      "Halpeney to. is foors is AResmDatrenter7 @realDonaldTrump wack hong? Thank you tessch my the stats!< @ChONewsene?< ant if in with Unnohleds \n",
      "\n",
      "\n",
      "Bean the tatidento @Jowsing inva lutpie.< ax Or dackele Opm my lool Abef the sere Ho conted Jectine suely.< That eftervertet sogated for. Lo\n",
      "\n",
      "\n",
      "Thankie I 22 @realDonaldTrump hald lisidervh. Methese, umper dood!<is beather Mey to nevud goon dough ceals. Them @N04<3, @plarsMonice.<30 c\n",
      "\n",
      "\n",
      "Goe URL   Trump.< off deal<ice propuld Coucty an strusir breg is enty coutee to @keAppecick suld enisst, ay o Theuthide Py Wip seronme the q\n",
      "\n",
      "\n",
      "Thow 3ery abluseg cxust ive eskByers ase Rejob @Mins.<.<uy, 2016<.I<!!<DETTL NY @ISarlNacker. Wig h.<cour Sucg Grichully a @realDonaldTrumb \n",
      "\n",
      "\n",
      "Avett!<owa reat 's anowien Foot! Shet @rulitiley, for monnas make Appress thist.<, in Uneld Cong sian Acrioustous to you the @Charszentradre\n",
      "\n",
      "\n",
      "But Pead The Stackuld and to it th amayy!<.Weur blaeds Ssing cormiating Pregrideve.<n will fils ef 21 fanyrfunt Sotias sheuz of a be beiegra\n",
      "\n",
      "\n",
      "Trump  Tovant 518 Was that any @xeald#Dongrennige yead misisouns greatios see on grat newesse! Hays Bote of seath oup< the de woulus I se ra\n",
      "\n",
      "\n",
      "W.< Ladesenchums mared of was me.RN wore. Whir, im, me chere. We pook  Jut guon..<. Trump!<o he pit what you plondiis lises Rame conewe Tran\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    a = rnn.predict2(x=rnn.mapper[\" \"], h=rnn.h, n=140)\n",
    "    l = \"\".join(rnn.word(a))\n",
    "    print(l+\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
