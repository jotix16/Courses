{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname('../'))\n",
    "\n",
    "from data_utils import video_to_frames\n",
    "from data_utils import metadata_loader\n",
    "from data_utils.kth_dataset_builder import DatasetBuilder\n",
    "\n",
    "from models.IMAGENET import Imagenet, Video_Feature_Extractor \n",
    "from models.IMAGENET import AVG_Video_Classifier, LSTM_Video_Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
<<<<<<< HEAD
     "output_type": "stream",
     "text": [
      "(16, 160, 160, 3) (6,)\n"
     ]
=======
     "text": "(16, 160, 160, 3)(6,)\n"
>>>>>>> e93287cc09f6792d66b3d2a453ef92f4d5b5da3a
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None, 160, 160, None), (6,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup builder\n",
    "video_path = '../data/kth-actions/video'\n",
    "frame_path = '../data/kth-actions/frame'\n",
    "builder = DatasetBuilder(video_path, frame_path, img_width=84, img_height=84, ms_per_frame=1000, max_frames=16)\n",
    "\n",
    "# Convert videos and generate metadata\n",
    "#builder.convert_videos_to_frames()\n",
    "metadata = builder.generate_metadata()\n",
    "\n",
    "# Build datasets\n",
    "train_ds = builder.make_video_dataset(metadata=metadata['train'])\n",
    "valid_ds = builder.make_video_dataset(metadata=metadata['valid'])\n",
    "\n",
    "# Preprocess dataset\n",
    "IMG_SIZE = 160 # All images will be resized to 160x160\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "def format_example(image, label):\n",
    "    image = tf.repeat(image,3,axis=3)   \n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(format_example)\n",
    "valid_ds = valid_ds.map(format_example)\n",
    "\n",
    "# Print\n",
    "for x, lab in valid_ds.take(1):\n",
    "    print(x.shape, lab.shape)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
<<<<<<< HEAD
     "output_type": "stream",
     "text": [
      "Training set, cases for each class: [70. 70. 70. 70. 70. 70.]\n",
      "Validation set, cases for each class: [15. 15. 15. 15. 15. 15.]\n"
     ]
=======
     "text": "Training set, cases for each class:[70. 70. 70. 70. 70. 70.]\nValidation set, cases for each class:[15. 15. 15. 15. 15. 15.]\n"
>>>>>>> e93287cc09f6792d66b3d2a453ef92f4d5b5da3a
    }
   ],
   "source": [
    "# Training set\n",
    "a = np.zeros(6)\n",
    "for _, label in train_ds.as_numpy_iterator():\n",
    "    a=a+label\n",
    "print(\"Training set, cases for each class:\",a)\n",
    "\n",
    "# Valid Set\n",
    "a = np.zeros(6)\n",
    "for _, label in valid_ds.as_numpy_iterator():\n",
    "    a=a+label\n",
    "print(\"Validation set, cases for each class:\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning \n",
    "### For videos\n",
    "Below we show to ways how to do transfer learning based on a pretrained base model.\n",
    "The only part that should be changed is the one comming after video_fature_extractor. Below we show to ways how one can use an RNN(LSTM) or a simple MLP to do the job.\n",
    "\n",
    "### For images\n",
    "If we want to train with frames as input there is no feature_extractor necessary. We can put a classifier directly on top of the base model.\n",
    "In order to see how we do fine tuning chacke the **Transfer_learning.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)RNN(LSTM) based classifier with Inception backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Activation, Dense, Conv3D, MaxPool3D, Flatten, Dropout, BatchNormalization, LSTM\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalAveragePooling2D, TimeDistributed\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "def My_Video_Classifier(features, class_nr, optimizer='adam'):\n",
    "    # model\n",
    "    full_model = tf.keras.Sequential([\n",
    "        features,\n",
    "        Dense(128, kernel_initializer=\"he_normal\"),\n",
    "        LSTM(512, input_shape=(None,128)),\n",
    "        #Dense(512, kernel_initializer=\"he_normal\"),\n",
    "        Dropout(rate=0.4),\n",
    "        Dense(class_nr)\n",
    "        ])\n",
    "    \n",
    "    #compile model\n",
    "    full_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_1 (Sequential)    (None, None, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 128)         262272    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 512)               1312768   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 23,380,902\n",
      "Trainable params: 1,578,118\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Base model (returns pretrained frozen base model trained on Imagenet)\n",
    "inception = Imagenet(input_shape=IMG_SHAPE, name='inception')\n",
    "\n",
    "# Feature Extractor (Has output (NR_FRAME x D) where D is feature dimension)\n",
    "featuer_ex = Video_Feature_Extractor(inception)\n",
    "\n",
    "# LSTM Clasifier\n",
    "model = My_Video_Classifier(features=featuer_ex, class_nr=6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds.shuffle(100).batch(25).prefetch(1), validation_data=valid_ds.batch(1), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_ds.batch(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)MLP classifier with Inception backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Activation, Dense, Conv3D, MaxPool3D, Flatten, Dropout, BatchNormalization, LSTM\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalAveragePooling2D, TimeDistributed\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# svm classifier\n",
    "def AVG_Video_Classifier(features, class_nr, optimizer='adam'):\n",
    "    # model\n",
    "    full_model = tf.keras.Sequential([\n",
    "        features,\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(2048, kernel_initializer=\"he_normal\"),\n",
    "        Dense(class_nr, kernel_initializer=\"he_normal\"),\n",
    "        ])\n",
    "    \n",
    "    #compile model\n",
    "    full_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return full_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model (returns pretrained frozen base model trained on Imagenet)\n",
    "inception = Imagenet(name='inception')\n",
    "\n",
    "# Feature Extractor (Has output (NR_FRAME x D) where D is feature dimension)\n",
    "featuer_ex = Video_Feature_Extractor(inception)\n",
    "\n",
    "# MLP Clasifier\n",
    "model = AVG_Video_Classifier(features=featuer_ex, class_nr=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds.shuffle(100).batch(25).prefetch(1),validation_data=valid_ds.batch(1), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_ds.batch(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Activation, Dense, Conv3D, MaxPool3D, Flatten, Dropout, BatchNormalization, LSTM\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalAveragePooling2D, TimeDistributed\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# svm classifier\n",
    "def AVG_Video_Classifier(features, class_nr, optimizer='adam'):\n",
    "    # model\n",
    "    full_model = tf.keras.Sequential([\n",
    "        features,\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(class_nr, kernel_regularizer=l2(0.001)),\n",
    "        ])\n",
    "\n",
    "    full_model.compile(loss='hinge',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model (returns pretrained frozen base model trained on Imagenet)\n",
    "inception = Imagenet(name='inception')\n",
    "\n",
    "# Feature Extractor (Has output (NR_FRAME x D) where D is feature dimension)\n",
    "featuer_ex = Video_Feature_Extractor(inception)\n",
    "\n",
    "# MLP Clasifier\n",
    "model = AVG_Video_Classifier(features=featuer_ex, class_nr=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds.shuffle(100).batch(25).prefetch(1),validation_data=valid_ds.batch(1), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_ds.batch(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bitdffe30996b2b4e2b97309fc8a703e620"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.9"
=======
   "version": "3.7.7-final"
>>>>>>> e93287cc09f6792d66b3d2a453ef92f4d5b5da3a
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
